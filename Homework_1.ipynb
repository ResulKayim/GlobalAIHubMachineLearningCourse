{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1 for Introduction to Machine Learning\n",
    "### Resul KAYIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q_1 How would you define Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is a sub-discipline of artificial intelligence. Machine learning is based on statistics and mathematical operations, and it is the computer modeling of systems that make inferences from data through these operations and make predictions on new samples through these inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q_2 What are the differences between Supervised and Unsupervised Learning? Specify example 3 algorithms for each of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning makes the necessary inferences on the data that has at least one label for each training example and gives an output using this for new situations.For example, we want to predict gender according to certain personality traits and it has features. We can train our model, which we will create using these examples, and as a result, our model, which learns the weights of the parameters, can make some predictions for new examples. We can give K-NN, SVM and Random Forest algorithms as examples to supervised learning.\n",
    "\n",
    "Unsupervised learning enables us to make inferences about the data by using the distances, neighborhood relationships and densities of the data samples without looking for any input-output, cause-effect relationship, without requiring our data set to have any tags. Suppose there are examples but we do not know exactly which body they are. In this case, using an unsupervised learning algorithm, taking into account the similarities between the samples, it can be distinguished into as many classes as desired and the sizes of our sweaters are determined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q_3 What are the test and validation set, and why would you want to use them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation dataset is a sample of data stored from our model's training that is used to give an estimate of model ability when adjusting the hyper-parameters of our model.\n",
    "\n",
    "The test set is the data set that the model has not encountered before, used to provide an unbiased evaluation of the final model that makes certain inferences from the training dataset.\n",
    "\n",
    "If we use the test set in the hyperparameters adjustment stage, our last success model will not be an absolute success because the samples that he has not seen before will not be used in the test stage. But if we use a validation set, we will test for things like this with a dataset that our model has never seen before, and the final result will show the true success of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q_4 What are the main preprocessing steps? Explain them in detail. Why we need to prepare our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With data preparation, the quality of the data set and the efficiency for the result can be increased by clearing the available data from unnecessary information and making estimates for missing information.\n",
    "\n",
    "   **Explarotary Data Analysis (EDA)** provides to perform initial investigations on data so as to discover patterns, to spot anomalies, to test hypothesis and to check assumptions with some methods.\n",
    "\n",
    "   **Preprocessing** is a very important step to prepare the data more appropriate for our model.\n",
    "    \n",
    "   **1. Duplicate Values:** There can be some duplicate values in every datasets and needed to be removed from it to increase the quality of the dataset.\n",
    "\n",
    "   **2. Imbalanced Data:** Imbalanced classification refers to a classification predictive modeling problem where the number of examples in the training dataset for each class label is not balanced.\n",
    "\n",
    "   **3. Missing Values:** In every dataset, there could be missing values at any points. In order to handle with this problem, there are two common method: *Eliminating the missing value* and *filling it with mean or medium.\n",
    "\n",
    "   **4. Outlier Detection:** Outliers are data points that do not belong to a certain population. It is an abnormal observation that lies far away from other values. Briefly, an outlier is an observation that diverges from otherwise well-structured data. There are five common methods to deal with it:<br>\n",
    "\n",
    "   **a. Standard Deviation:** If there are any data points that are more than 3 times the standard deviation, then those points are very likely to be anomalous or outliers.\n",
    "\n",
    "   **b. Boxplots:** Box plots are a graphical depiction of numerical data through their quantiles. Any data points that show above or below the whiskers, can be considered outliers or anomalous.\n",
    "    \n",
    "   **c. DBScan Clustering:** DBScan is a clustering algorithm used cluster data into groups. It is also used as a density-based anomaly detection method with either single or multi-dimensional data. It is a more complicated method than standard deviation and boxplots.\n",
    "\n",
    "   **d. Isolation Forest:** Isolation Forest is an unsupervised learning algorithm that belongs to the ensemble decision trees family. It is a different method from others. It explicitly isolates anomalies instead of profiling and constructing normal points and regions by assigning a score to each data point.\n",
    "\n",
    "   **e. Robust Random Cut Forest:** Random Cut Forest (RCF) algorithm is Amazonâ€™s unsupervised algorithm for detecting anomalies. It works by associating an anomaly score as well. Low score values indicate that the data point is considered as normal. High values indicate the presence of an anomaly in the data.*<br>\n",
    "\n",
    "   **5. Feature Scaling:** There are two types of scaling the features:<br>\n",
    "    \n",
    "   **a. Standardization:** The new value is defined as dependent on the standard deviation and the mode of the data. After this process, standard deviation of that data becomes 1 and mean becomes 0.\n",
    "        \n",
    "   **b. Normalization:** The new value is dependent on:\n",
    "            when the feature is more or less uniformly distributed across a fixed range: linear scaling **->** maximum and minimum values of data inputs\n",
    "            when the feature contains extreme outliers: clipping **->** maximum and minimum values of data inputs\n",
    "            when the feature conforms to the power law: log scaling **->** logarithms of data inputs\n",
    "            when the feature distribution does not contain extreme values: z-score **->** the standard deviation and the mode of the data\n",
    "    \n",
    "   **6. Bucketing (Binning):** It is an important method used to minimize the effects of noisy data</ins>. The actual data values are divided into small intervals known as bins and then they are replaced by a general value calculated for that bin.\n",
    "\n",
    "   **7. Feature Encoding:** Some transformations can be performed on the data so that it can be easily accepted as input for machine learning algorithms. There are two common types of feature encoding:\n",
    "\n",
    "   **a. Nominal:** Any one-to-one mapping can be done which retains the meaning. One-hot-encoding can be applied to this kind of values if it is given as input, otherwise we can convert them directly by using LabelEncoder.\n",
    "   \n",
    "   Example: If city is a input value => Turkey, Germany, US, Italy...\n",
    "   \n",
    "  **b. Ordinal:** An order-preserving change of values.\n",
    "  \n",
    "  Example: Very Bad(0), Bad(1), Good(2), Very Good(3)\n",
    "    \n",
    "   **8. Train/Validation/Test Split:** There are the explanation of types of dataset above. The split process is very important in data preparation. Available data refers to training dataset and test dataset while new available data refers to training dataset and validation dataset. It is more accurate to use validation dataset together with others. The percentage of training dataset should be more than the other types.\n",
    "\n",
    "   **9. Cross Validation:** Testing a prediction function on the same data</ins> is a critical mistake. It is called overfitting. To avoid it, the most effective way is to hold out part of the available data as a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q_5How you can explore and analyse countionus and discrete variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete data involves round, concrete numbers that are determined by counting while continuous data involves complex numbers that are measured across a specific time interval. That can be said discrete variables are countable and finite but continuous variables are uncountable and infinite (take an infinite time to count).\n",
    "\n",
    "Example of discrete variable: The number of member in a family. Example of continuous variable: The annual speed of a car (as a distinct line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q_6 Analyse the plot given below. (What is the plot and variable type, check the distribution and make comment about how you can preproccess it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a graph showing flower width on the horizontal axis and their proportions on the vertical axis. This graph consist of a curve (distinct line) so the type of data variable is continuous. Rises and drops are observed according to the rate of change of the distribution.\n",
    "\n",
    "Process:\n",
    "\n",
    "Sorting into Balanced and Imbalanced Data -> Balancing the Imbalanced Data -> Fitting Missing Values with the Median -> Cleaning Noise Data -> Standardization and Normalization -> Feature Extraction (PCA) -> Dataset Split -> Ready"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
